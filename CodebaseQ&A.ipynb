{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q fastapi uvicorn pyngrok gitpython transformers torch langchain langchain-community chromadb bitsandbytes nest_asyncio\n",
        "\n",
        "# ============================\n",
        "# 2Ô∏è‚É£ Imports\n",
        "# ============================\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from fastapi.responses import RedirectResponse\n",
        "from pydantic import BaseModel\n",
        "from pathlib import Path\n",
        "import tempfile\n",
        "import torch\n",
        "from git import Repo\n",
        "from typing import List, Optional\n",
        "\n",
        "# LangChain imports\n",
        "try:\n",
        "    from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
        "    from langchain_community.vectorstores import Chroma\n",
        "    from langchain_community.llms import HuggingFacePipeline\n",
        "except Exception:\n",
        "    from langchain.document_loaders import DirectoryLoader, TextLoader\n",
        "    from langchain.vectorstores import Chroma\n",
        "    from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.embeddings.base import Embeddings\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# ============================\n",
        "# 3Ô∏è‚É£ Embeddings\n",
        "# ============================\n",
        "class HFEncoder(Embeddings):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = \"BAAI/bge-small-en-v1.5\",\n",
        "        device: Optional[str] = None,\n",
        "        batch_size: int = 16,\n",
        "        normalize: bool = True,\n",
        "        max_length: int = 512,\n",
        "    ):\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
        "        self.batch_size = batch_size\n",
        "        self.normalize = normalize\n",
        "        self.max_length = max_length\n",
        "        self.doc_prefix = \"passage: \"\n",
        "        self.query_prefix = \"query: \"\n",
        "\n",
        "    def _mean_pooling(self, model_output, attention_mask):\n",
        "        token_embeddings = model_output.last_hidden_state\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "        summed = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
        "        counts = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
        "        return summed / counts\n",
        "\n",
        "    def _encode_texts(self, texts: List[str]) -> List[List[float]]:\n",
        "        vecs = []\n",
        "        for i in range(0, len(texts), self.batch_size):\n",
        "            batch = texts[i : i + self.batch_size]\n",
        "            enc = self.tokenizer(batch, padding=True, truncation=True, max_length=self.max_length, return_tensors=\"pt\").to(self.device)\n",
        "            with torch.no_grad():\n",
        "                out = self.model(**enc)\n",
        "            pooled = self._mean_pooling(out, enc[\"attention_mask\"])\n",
        "            if self.normalize:\n",
        "                pooled = torch.nn.functional.normalize(pooled, p=2, dim=1)\n",
        "            vecs.extend(pooled.cpu().tolist())\n",
        "        return vecs\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        texts = [self.doc_prefix + t for t in texts]\n",
        "        return self._encode_texts(texts)\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        text = self.query_prefix + text\n",
        "        return self._encode_texts([text])[0]\n",
        "\n",
        "# ============================\n",
        "# 4Ô∏è‚É£ LLM loader\n",
        "# ============================\n",
        "def get_llm():\n",
        "    model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "    tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "    load_kwargs = {}\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            load_kwargs = dict(device_map=\"auto\", torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
        "            import bitsandbytes as _bnb\n",
        "            load_kwargs.update(dict(load_in_4bit=True))\n",
        "        except Exception:\n",
        "            pass\n",
        "    else:\n",
        "        load_kwargs = dict(device_map=None, torch_dtype=torch.float32)\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, **load_kwargs)\n",
        "    gen_pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tok,\n",
        "        max_new_tokens=700,\n",
        "        do_sample=False,\n",
        "        temperature=0.1,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.05,\n",
        "        pad_token_id=tok.eos_token_id,\n",
        "    )\n",
        "    return HuggingFacePipeline(pipeline=gen_pipe), tok\n",
        "\n",
        "# ============================\n",
        "# 5Ô∏è‚É£ Prompts\n",
        "# ============================\n",
        "EXPLAIN_PROMPT = PromptTemplate(\n",
        "    template=(\n",
        "        \"You are a helpful assistant that explains code clearly.\\n\\n\"\n",
        "        \"User Question:\\n{question}\\n\\n\"\n",
        "        \"Relevant Code Snippets:\\n{context}\\n\\n\"\n",
        "        \"Instructions:\\n\"\n",
        "        \"- Explain what the code does in simple terms.\\n\"\n",
        "        \"- If asking about a whole file, cover purpose, key functions/components, and how it fits the app.\\n\"\n",
        "        \"- If asking about a specific function or component, explain its logic step-by-step.\\n\"\n",
        "        \"- Avoid just repeating the code; provide reasoning.\\n\\n\"\n",
        "        \"Answer:\"\n",
        "    ),\n",
        "    input_variables=[\"question\", \"context\"],\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# 6Ô∏è‚É£ Repo utils\n",
        "# ============================\n",
        "def get_repo(user_input: str) -> str:\n",
        "    if user_input.startswith(\"http\") and \"github.com\" in user_input:\n",
        "        tmp = tempfile.mkdtemp()\n",
        "        Repo.clone_from(user_input, tmp)\n",
        "        return tmp\n",
        "    p = Path(user_input)\n",
        "    if p.exists():\n",
        "        return str(p.resolve())\n",
        "    raise ValueError(\"‚ùå Invalid input! Provide a local path or a GitHub URL.\")\n",
        "\n",
        "def load_documents(repo_path: str):\n",
        "    patterns = [\"**/*.js\", \"**/*.jsx\", \"**/*.ts\", \"**/*.tsx\", \"**/*.py\", \"**/*.md\"]\n",
        "    docs = []\n",
        "    for pat in patterns:\n",
        "        loader = DirectoryLoader(repo_path, glob=pat, loader_cls=TextLoader, show_progress=True)\n",
        "        try:\n",
        "            docs.extend(loader.load())\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Skipped pattern {pat}: {e}\")\n",
        "    if not docs:\n",
        "        raise ValueError(\"No documents found in repository.\")\n",
        "    return docs\n",
        "\n",
        "def split_docs(documents):\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1200,\n",
        "        chunk_overlap=150,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
        "    )\n",
        "    return splitter.split_documents(documents)\n",
        "\n",
        "def build_db(docs):\n",
        "    embeddings = HFEncoder(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "    db = Chroma.from_documents(docs, embeddings, persist_directory=\"./chroma_db\")\n",
        "    db.persist()\n",
        "    return db\n",
        "\n",
        "def build_explainer_qa(db, llm):\n",
        "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
        "    qa = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        chain_type=\"stuff\",\n",
        "        chain_type_kwargs={\"prompt\": EXPLAIN_PROMPT},\n",
        "        return_source_documents=False,\n",
        "    )\n",
        "    return qa\n",
        "\n",
        "def find_file_anywhere(repo_root: str, query_path: str) -> Optional[str]:\n",
        "    q = query_path.strip().replace(\"\\\\\", \"/\").lower()\n",
        "    candidates = []\n",
        "    for p in Path(repo_root).rglob(\"*\"):\n",
        "        if p.is_file():\n",
        "            rel = str(p.relative_to(repo_root)).replace(\"\\\\\", \"/\").lower()\n",
        "            abs_p = str(p.resolve())\n",
        "            if rel.endswith(q) or Path(rel).name == Path(q).name:\n",
        "                candidates.append(abs_p)\n",
        "    candidates.sort(key=lambda x: len(x))\n",
        "    return candidates[0] if candidates else None\n",
        "\n",
        "def chunk_text_by_tokens(text: str, tok: AutoTokenizer, max_context_tokens: int, reserve_for_prompt: int = 2000) -> List[str]:\n",
        "    max_tokens = max(512, max_context_tokens - reserve_for_prompt)\n",
        "    ids = tok.encode(text, add_special_tokens=False)\n",
        "    chunks = []\n",
        "    for i in range(0, len(ids), max_tokens):\n",
        "        sub = ids[i: i + max_tokens]\n",
        "        chunks.append(tok.decode(sub))\n",
        "    return chunks\n",
        "\n",
        "def explain_file(file_query: str, repo_root: str, llm: HuggingFacePipeline, tok: AutoTokenizer) -> str:\n",
        "    target = find_file_anywhere(repo_root, file_query)\n",
        "    if not target:\n",
        "        return f\"‚ùå File not found: {file_query}\"\n",
        "\n",
        "    with open(target, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        code = f.read()\n",
        "\n",
        "    max_ctx = getattr(llm.pipeline.model.config, \"max_position_embeddings\", 32768)\n",
        "    chunks = chunk_text_by_tokens(code, tok, max_context_tokens=max_ctx, reserve_for_prompt=1500)\n",
        "\n",
        "    if len(chunks) == 1:\n",
        "        prompt = (\n",
        "            f\"Explain the following file in detail.\\n\\n\"\n",
        "            f\"File: {target}\\n\\n\"\n",
        "            f\"{code}\\n\\n\"\n",
        "            f\"Instructions:\\n\"\n",
        "            f\"- Purpose of the file and how it fits the project\\n\"\n",
        "            f\"- Key functions/components and their logic\\n\"\n",
        "            f\"- Any notable patterns, libraries, and data flow\"\n",
        "        )\n",
        "        return llm(prompt)\n",
        "\n",
        "    parts = []\n",
        "    for idx, ch in enumerate(chunks, 1):\n",
        "        prompt = (\n",
        "            f\"Explain PART {idx} of this file clearly (out of {len(chunks)} parts). \"\n",
        "            f\"Focus on what this part does and how it connects to the rest.\\n\\n\"\n",
        "            f\"File: {target}\\n\\n\"\n",
        "            f\"{ch}\\n\\n\"\n",
        "            f\"Return a concise explanation for this part.\"\n",
        "        )\n",
        "        parts.append(llm(prompt))\n",
        "\n",
        "    synthesizer = (\n",
        "        \"You are given multiple part-wise explanations of a source file. \"\n",
        "        \"Combine them into a single coherent explanation that reads like one document. \"\n",
        "        \"Include: file purpose, key components/functions, data flow, and how it fits in the project.\\n\\n\"\n",
        "        \"Parts:\\n\" + \"\\n\\n\".join(f\"- Part {i+1}: {p}\" for i, p in enumerate(parts))\n",
        "    )\n",
        "    final = llm(synthesizer)\n",
        "    return f\"üìÑ Explanation of {target}:\\n\\n{final}\"\n",
        "\n",
        "# ============================\n",
        "# 7Ô∏è‚É£ FastAPI setup\n",
        "# ============================\n",
        "app = FastAPI(title=\"Codebase Explainer API\", description=\"Upload repo and ask questions\", version=\"1.0.0\")\n",
        "app.add_middleware(CORSMiddleware, allow_origins=[\"*\"], allow_credentials=True, allow_methods=[\"*\"], allow_headers=[\"*\"])\n",
        "\n",
        "@app.get(\"/\", include_in_schema=False)\n",
        "def root():\n",
        "    return RedirectResponse(url=\"/docs\")\n",
        "\n",
        "class AskRequest(BaseModel):\n",
        "    question: str\n",
        "\n",
        "llm, tok = get_llm()\n",
        "explainer_chain = None\n",
        "db = None\n",
        "repo_root = None\n",
        "\n",
        "# ============================\n",
        "# 8Ô∏è‚É£ Endpoints\n",
        "# ============================\n",
        "@app.post(\"/load_repo\")\n",
        "def load_repo_endpoint(repo_url: str):\n",
        "    global db, explainer_chain, repo_root\n",
        "    try:\n",
        "        repo_root = get_repo(repo_url)\n",
        "        documents = load_documents(repo_root)\n",
        "        docs = split_docs(documents)\n",
        "        db = build_db(docs)\n",
        "        explainer_chain = build_explainer_qa(db, llm)\n",
        "        return {\"status\": \"success\", \"repo_root\": repo_root}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "@app.post(\"/ask\")\n",
        "def ask_question(req: AskRequest):\n",
        "    if not explainer_chain or not repo_root:\n",
        "        raise HTTPException(status_code=400, detail=\"Repo not loaded yet. Call /load_repo first.\")\n",
        "    q = req.question.strip()\n",
        "    if q.lower().startswith(\"explain file\"):\n",
        "        parts = q.split(\" \", 2)\n",
        "        if len(parts) < 3:\n",
        "            raise HTTPException(status_code=400, detail=\"Usage: explain file <relative/or/partial/path>\")\n",
        "        file_query = parts[2]\n",
        "        ans = explain_file(file_query, repo_root, llm, tok)\n",
        "    else:\n",
        "        ans = explainer_chain.run(q)\n",
        "    return {\"answer\": ans}\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health():\n",
        "    return {\"status\": \"ok\"}\n",
        "\n",
        "# ============================\n",
        "# 9Ô∏è‚É£ Run FastAPI via ngrok\n",
        "# ============================\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# üîë Set your ngrok token (replace with your actual token)\n",
        "ngrok.set_auth_token(\"32QsCHAh1caySQL0nyEFubn3CmG_81behFVFxpRx3b5xHG8Vc\")\n",
        "\n",
        "# Start a tunnel on port 8000\n",
        "port = 8000\n",
        "public_url = ngrok.connect(port)\n",
        "\n",
        "print(f\"üîó Public URL: {public_url}/api (Swagger UI)\")\n",
        "\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=port)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdDf-dQnNYmQ",
        "outputId": "b8d43694-03e0-4e81-e14f-78c6d208f196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-1' coro=<Server.serve() done, defined at /usr/local/lib/python3.12/dist-packages/uvicorn/server.py:69> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/main.py\", line 580, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 67, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 396, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 303, in __step\n",
            "    self.__step_run_and_handle_result(exc)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 70, in serve\n",
            "    with self.capture_signals():\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 331, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîó Public URL: NgrokTunnel: \"https://5dbddf6dfd40.ngrok-free.app\" -> \"http://localhost:8000\"/api (Swagger UI)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [304]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     39.49.113.139:0 - \"GET / HTTP/1.1\" 307 Temporary Redirect\n",
            "INFO:     39.49.113.139:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     39.49.113.139:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "INFO:     39.49.113.139:0 - \"POST /docs/load_repo HTTP/1.1\" 404 Not Found\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 3116.70it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<00:00, 6429.69it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2603.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     39.49.113.139:0 - \"POST /load_repo?repo_url=https%3A%2F%2Fgithub.com%2Fhassan-jamshaid10%2FPortfolio-Fullstack.git HTTP/1.1\" 200 OK\n",
            "INFO:     39.49.113.139:0 - \"POST /load_repo HTTP/1.1\" 422 Unprocessable Entity\n",
            "INFO:     39.49.113.139:0 - \"POST /load_repo HTTP/1.1\" 422 Unprocessable Entity\n",
            "INFO:     39.49.113.139:0 - \"POST /load_repo HTTP/1.1\" 422 Unprocessable Entity\n",
            "INFO:     39.49.113.139:0 - \"POST /load_repo HTTP/1.1\" 422 Unprocessable Entity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1690.06it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<00:00, 6367.87it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 1216.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     39.49.113.139:0 - \"POST /load_repo?repo_url=https://github.com/hassan-jamshaid10/Portfolio-Fullstack.git HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1656.19it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<00:00, 6096.37it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2774.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     39.49.113.139:0 - \"POST /load_repo?repo_url=https%3A%2F%2Fgithub.com%2Fhassan-jamshaid10%2FPortfolio-Fullstack.git HTTP/1.1\" 200 OK\n",
            "INFO:     39.49.113.139:0 - \"OPTIONS /ask HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3455447835.py:295: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  ans = explainer_chain.run(q)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     39.49.113.139:0 - \"POST /ask HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 2037.31it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<00:00, 5209.78it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 1515.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     39.49.113.139:0 - \"POST /load_repo?repo_url=https%3A%2F%2Fgithub.com%2Fhassan-jamshaid10%2FPortfolio-Fullstack.git HTTP/1.1\" 200 OK\n",
            "INFO:     39.49.113.139:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
            "INFO:     39.49.113.139:0 - \"OPTIONS /ask HTTP/1.1\" 200 OK\n",
            "INFO:     39.49.113.139:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
            "INFO:     39.49.113.139:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
            "INFO:     39.49.113.139:0 - \"POST /ask HTTP/1.1\" 200 OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SXTohaBMPaq3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}